{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "For this project, I am using something related to my work to see if it is possible to classify searches in the [NCBI Nucleotide database]('https://ncbi.nlm.nih.gov/nucleotide') based on whether a user's visit was a successful one or not (a visit is defined as a period activity within a browser that is continuous; unless 30m pass without any user activity, the activity is retained in the same visit).\n",
    "\n",
    "NCBI is working on improving the search experience, and one goal of that project is to provide knowledge panels or search suggestions that are more likely to lead users to good (or better) results.  Searching for sequences in the Nucleotide database can be very challenging.  The NCBI search infrastrucutre, with the exception of PubMed, is not like that of conventional search engines, the dominate form of search and retrieval processes when using computers, and we are hoping to minimize those differences.\n",
    "\n",
    "NCBI is currently conducting surveys, and those surveys will provide ground-truth labels for whether a given visit was successful.  In the meantime, for this project, I employ surrogate calsssification labels based on the assumption that the primary goal is to download DNA or RNA sequences from Nucleotide.  So we use weblogs to identify visits that did or did not download a sequence as our 'success' and 'failure' labels, respectively.\n",
    "\n",
    "# 1. Get Data from AppLog Client\n",
    "NCBI has an internal web application logging system that records what users search, what pages they view, and what they click on.  We use this ssytem to extract the precise set of data we need for a given analytical task.  For this particular task, we can summarize what we need need to do:\n",
    "* extract a series of searches from the Nucleotide database, identifying which pages gave those searches, and what visit identifiers they belong to\n",
    "* use those searches as an input to extract the entire visit activity for that search\n",
    "* combine those data into one indexed dataset\n",
    "\n",
    "**NOTE**: This can only be performed at NCBI internally, so the code for data generatation are not executed here, they are generated by executing the get_data.py script in [the repo for this project]('https://github.com/kharo/BIOF509_Final_Project').  Most of this script utilizes command line arguments, but is it included for your contextual understanding.  The data were moved to a public folder in a dropbox account, so we download the data directly via API for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Data from Dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use latest pandas to directly download raw csv content\n",
    "import os\n",
    "import pandas as pd\n",
    "cwd = os.getcwd()\n",
    "url = 'https://dl.dropbox.com/s/njwmzeb04op9ymz/visit_data.txt?dl=0'  \n",
    "data = pd.read_csv(url, sep='\\t', header=0, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Randomly Subset Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# choose the number of visits to analyze - 1K works well for quick verification/illustrative purposes\n",
    "# the results section will discuss results from 50K visits.\n",
    "num_samples = 1000\n",
    "visits = list(set(data['vid']))\n",
    "random.seed(21)\n",
    "random_visits = random.sample(visits, num_samples)\n",
    "data = data[data['vid'].isin(random_visits)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Feature Functions\n",
    "What is returned from our internal AppLog extraction script is a large matrix of metadata that describes each page render and click for each visit that contained a nucleotide search in the month of October.  We have also returned a column that identifies those original searches, which has been named **anchor_page**\n",
    "\n",
    "We leverage the data contained in these columns to create features when certain conditions are satisfied.  The features describe certain aspects of a search at NCBI.  All but one feature are based on the search text entered by the user.  The other feature uses the number of results returned from the search.  The features are not randomly made - they are based on a year's worth of other analytical work that suggested there is some sort of correlation between specific user actions and the values of these features.\n",
    "\n",
    "In the cell below we define the functions to create features and some additional necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_org(search, org_list):\n",
    "    '''\n",
    "    this function examines a search string and checks for the presence of terms\n",
    "    that are in a list containing popular model organism names\n",
    "    :param search: string for the user's search\n",
    "    :param org_list: a list of strings containing organism names\n",
    "    :return Bool indicating presence/absence of term\n",
    "    '''\n",
    "    s = search.split()\n",
    "    if any([term in org_list for term in s]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def has_gene(search, gene_list):\n",
    "    '''\n",
    "    this function examines a search string and checks for the presence of terms that are in a list \n",
    "    containing organism gene names from an internal NCBI database\n",
    "    :param search: string for the user's search\n",
    "    :param gene_list: a list of strings containing organism gene symbols, names and more\n",
    "    :return Bool indicating presence/absence of term\n",
    "    '''\n",
    "    s = search.split()\n",
    "    if any([term in gene_list for term in s]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def has_boolean(search):\n",
    "    '''\n",
    "    looks in string for AND or OR in query to see if user did a boolean search\n",
    "    :param schear: the string for the user's search\n",
    "    :return bool: the category for the value\n",
    "    '''\n",
    "    if ' and ' in search:\n",
    "        return True\n",
    "    elif ' or ' in search:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def has_biol_term(search, term_list):\n",
    "    '''\n",
    "    this function looks for specific, custom biological terms set we have used in previous analyses in the user's search\n",
    "    :param search: string for the user's search\n",
    "    :param term_list: a list of strings containing special field-specific biology terms\n",
    "    :return Bool indicating presence/absence of term\n",
    "    '''\n",
    "    s = search.split()\n",
    "    if any([term in term_list for term in s]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def term_count(search):\n",
    "    '''\n",
    "    returns the number of terms from a search using one of three levels\n",
    "    :param search: string for user's search\n",
    "    :return str: the category for the value found from counting the number of terms in the search\n",
    "    '''\n",
    "    c = len(search.split())\n",
    "    if c < 2:\n",
    "        return 'low'\n",
    "    elif c < 5:\n",
    "        return 'med'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "def str_length(search):\n",
    "    '''\n",
    "    bins the number of characters in the search string and assigns to one of three levels\n",
    "    :param search: the search string\n",
    "    :return str: the category for the value\n",
    "    '''\n",
    "    try:\n",
    "        c = len(search)\n",
    "    except:\n",
    "        return 'low'\n",
    "    if c < 10:\n",
    "        return 'low'\n",
    "    elif c < 30:\n",
    "        return 'med'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "def bin_result_count(count):\n",
    "    '''\n",
    "    bins the number of search results from a given query into three levels\n",
    "    :param count: the integer value of result counts from the search\n",
    "    :return str: the category for the value\n",
    "    '''\n",
    "    try:\n",
    "        c = int(count)\n",
    "    except:\n",
    "        c = 0\n",
    "    if c < 20:\n",
    "        return 'low'\n",
    "    elif c < 200:\n",
    "        return 'med'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "def has_accession(search, patterns):\n",
    "    '''\n",
    "    looks for an accession number in the query\n",
    "    :param search: string for the user's search\n",
    "    :param patterns: list of regex patterns used to define accession numbers for sequences at NCBI\n",
    "    :return: Boolean, whether a regex for an accession number is in query\n",
    "    '''\n",
    "    indicator = False\n",
    "    for word in search.split():\n",
    "        if any([p.search(word) for p in patterns]):\n",
    "            indicator = True\n",
    "            break\n",
    "\n",
    "    if indicator:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def has_identifier(search):\n",
    "    '''\n",
    "    looks for a GI identifier in the query, specific to NCBI\n",
    "    :param search: string for the user's search\n",
    "    :return: Boolean, whether search is all numeric and thus likely a GI\n",
    "    '''\n",
    "    sL = search.split()\n",
    "    if len(sL) == 1:\n",
    "        chars = set('0123456789uid.')\n",
    "        if all((c in chars) for c in sL[0]):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def has_sequence(search):\n",
    "    '''\n",
    "    looks for a genetic sequence in the query\n",
    "    :param search: string for the user's search\n",
    "    :return: Boolean, whether a sequence was used in the search\n",
    "    '''\n",
    "    chars = set('atcgu')\n",
    "    indicator = False\n",
    "    # split search into terms\n",
    "    for word in search.split():\n",
    "        if all((c in chars) for c in word) and len(word) > 10:\n",
    "            indicator = True\n",
    "            break\n",
    "        # this regex is a special identifier for a sequence at NCBI\n",
    "        elif re.compile(\"chr\\d:\\d\").search(word):\n",
    "            indicator = True\n",
    "            break\n",
    "    return indicator\n",
    "\n",
    "def has_taxid(search):\n",
    "    '''\n",
    "    looks for a tax ID specifier in the query\n",
    "    :param search: string for the user's search\n",
    "    :return: Boolean, whether a taxid is in query\n",
    "    '''\n",
    "    indicator = False\n",
    "    p = re.compile(\"txid\\d\")\n",
    "    # split search into terms\n",
    "    for word in search.split():\n",
    "        if p.search(word):\n",
    "            indicator = True\n",
    "            break\n",
    "    if indicator:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "import re\n",
    "# this is a special set of patterns NCBI uses to recognize accession numbers\n",
    "acc_patterns = []\n",
    "acc_patterns.append(re.compile(\"^[a-z]{1}\\d{5}\\\\b\"))\n",
    "acc_patterns.append(re.compile(\"^[a-z]{2}\\d{6}\\\\b\"))\n",
    "acc_patterns.append(re.compile(\"^[a-z]{3}\\d{5}\\\\b\"))\n",
    "acc_patterns.append(re.compile(\"^[a-z]{4}\\d{2}\\\\b\"))\n",
    "acc_patterns.append(re.compile(\"^[a-z]{4}\\d{8}\\\\b\"))\n",
    "acc_patterns.append(re.compile(\"^[a-z]{4}\\d{10}\\\\b\"))\n",
    "acc_patterns.append(re.compile(\"^[a-z]{5}\\d{7}\\\\b\"))\n",
    "acc_patterns.append(re.compile(\"^[a-z]{2}_\"))\n",
    "acc_patterns.append(re.compile(\"^[a-z]{3}_\"))\n",
    "acc_patterns.append(re.compile(\"^[a-z]{3}\\s\\d{9}\\\\b\"))\n",
    "acc_patterns.append(re.compile(\"^samn\\d\\d\"))\n",
    "acc_patterns.append(re.compile(\"^prjna\\d\\d\"))\n",
    "acc_patterns.append(re.compile(\"^pnusas\\d\\d\"))\n",
    "acc_patterns.append(re.compile(\"^samea\\d\\d\"))\n",
    "acc_patterns.append(re.compile(\"^cfsan\\d\\d\"))   \n",
    "acc_patterns.append(re.compile(\"^gtex\\.\"))\n",
    "acc_patterns.append(re.compile(\"^asm\\d{5}[a-z]{1}\\d{1}\\\\b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load Libraries from Dropbox and Execute Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have stored on file in the same dropbox directory a matrix of term lists\n",
    "# we convert each relevant row into a set to pass to the functions that require them\n",
    "url = 'https://dl.dropbox.com/s/d1hosy5g8ztr0ql/terms.txt?dl=0'  \n",
    "terms = pd.read_csv(url, sep='\\t', header=0, dtype=str, na_filter=False)\n",
    "term_sets = {}\n",
    "for col in terms:\n",
    "    # the file has unequal column lengths so we drop the NaN entries before creating the set\n",
    "    term_sets[col] = set(x.lower() for x in terms[col].tolist() if x != '')\n",
    "\n",
    "# ensure all searches are strings to avoid string/float errors\n",
    "data['search_text'] = data['search_text'].apply(str)\n",
    "\n",
    "# apply feature functions to those that need additional arguments (NOTE: one function uses a count column)\n",
    "data['org'] = data.apply(lambda x: has_org(x['search_text'], term_sets['org_list']), axis=1)\n",
    "data['gene'] = data.apply(lambda x: has_gene(x['search_text'], term_sets['gene_list']), axis=1)\n",
    "data['bio_term'] = data.apply(lambda x: has_biol_term(x['search_text'], term_sets['term_list']), axis=1)\n",
    "data['has_acc'] = data.apply(lambda x: has_accession(x['search_text'], acc_patterns), axis=1)\n",
    "\n",
    "# these features do not require additional arguments\n",
    "data['bool'] = data.apply(lambda x: has_boolean(x['search_text']), axis=1)\n",
    "data['length'] = data.apply(lambda x: str_length(x['search_text']), axis=1)\n",
    "data['result_count'] = data.apply(lambda x: bin_result_count(x['count']), axis=1)\n",
    "data['has_id'] = data.apply(lambda x: has_identifier(x['search_text']), axis=1)\n",
    "data['has_seq'] = data.apply(lambda x: has_sequence(x['search_text']), axis=1)\n",
    "data['has_tax'] = data.apply(lambda x: has_taxid(x['search_text']), axis=1)\n",
    "data['term_count'] = data.apply(lambda x: term_count(x['search_text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Label Generation\n",
    "Similar to features, we use the data within a visit to create a success or failure label.  Intellectually speaking, we *would* define success as someone who comes to NCBI and finds what they want.  We are working on obtaining those data currently via surveys.  For the purposes of this analysis, we create artifical features here.\n",
    "\n",
    "We will define success based on institutional knowledge and user interviews as someone who viewed (FASTA) or downloaded a sequence from a Nucleotide record.  Eventuslly, our group plans to use user surveys to define a success or failure based on answers from this question: \"Did you find what you were looking for today?\".  In the meantime, the surrogate success label executes the following logic:\n",
    "* subset data for each visit\n",
    "* find the first \"anchor search\" in each visit, after time sorting (some visits may have more than one search, so we only use the first one if that is the case)\n",
    "* examine all rows after that search to look for matches to the conditions which specify a successful visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docsum_fasta(row, col1, col2, col3, col4):\n",
    "    '''\n",
    "    this function looks for conditions that mean someone clicked to see a FASTA record directly from a docsum page\n",
    "    :param col1: str for column with jsevent\n",
    "    :param col2: str for column with the NCBI database\n",
    "    :param col3: str for column with the NCBI page type\n",
    "    :param col4: str for column with the link source attribute\n",
    "    '''\n",
    "    indicator = False\n",
    "    if (\n",
    "        row[col1] == 'click' and row[col2] == 'nuccore' and row[col3] == 'docsum'\n",
    "        and row[col4] == 'docsum_fasta' \n",
    "    ):\n",
    "        indicator =  True\n",
    "    return indicator\n",
    "\n",
    "def genbank_to_fasta(row, col1, col2, col3, col4):\n",
    "    '''\n",
    "    this function looks for conditions that mean someone switched to FASTA view from GenBank view on a record page\n",
    "    :param col1: str for column with jsevent\n",
    "    :param col2: str for column with the NCBI database\n",
    "    :param col3: str for column with the NCBI page type\n",
    "    :param col5: str for column with the link text\n",
    "    '''\n",
    "    indicator = False\n",
    "    if (\n",
    "        row[col1] == 'click' and row[col2] == 'nuccore' and row[col3] == 'genbank'\n",
    "         and row[col4] == 'fasta'\n",
    "    ):\n",
    "        indicator =  True\n",
    "    \n",
    "    return indicator\n",
    "\n",
    "def download_docsum(row, col1, col2, col3, col4):\n",
    "    '''\n",
    "    this function looks for conditions that mean someone downloaded records from a docsum page\n",
    "    :param col1: str for column with jsevent\n",
    "    :param col2: str for column with the NCBI database\n",
    "    :param col3: str for column with the NCBI page type\n",
    "    :param col4: str for column with the link_action_name attribute\n",
    "    '''\n",
    "    indicator = False\n",
    "    if (\n",
    "        row[col1] == 'click' and row[col2] == 'nuccore' and row[col3] == 'docsum'\n",
    "        and row[col4] == 'create file'\n",
    "    ):\n",
    "        indicator =  True\n",
    "    \n",
    "    return indicator\n",
    "    \n",
    "def download_record(row, col1, col2, col3, col4):\n",
    "    '''\n",
    "    this function looks for conditions that mean someone downloaded a FASTA record from a record page\n",
    "    :param col1: str for column with jsevent\n",
    "    :param col2: str for column with the NCBI database\n",
    "    :param col3: str for column with the NCBI page type\n",
    "    :param col4: str for column with the link_action_name attribue\n",
    "    '''\n",
    "    indicator = False\n",
    "    if (\n",
    "        row[col1] == 'click' and row[col2] == 'nuccore' and row[col3] in ['genbank', 'fasta', 'summary', 'asn.1']\n",
    "        and row[col4] == 'create file'\n",
    "    ):\n",
    "        indicator =  True\n",
    "\n",
    "    return indicator\n",
    "\n",
    "# provide the column names needed for each function\n",
    "col_names_docsum_fasta = ('jsevent', 'ncbi_db', 'ncbi_pdid', 'link_src')\n",
    "col_names_genbank_fasta = ('jsevent', 'ncbi_db', 'ncbi_pdid', 'link_text')\n",
    "col_names_download_docsum = ('jsevent', 'ncbi_db', 'ncbi_pdid', 'link_action_name')\n",
    "col_names_download_record = ('jsevent', 'ncbi_db', 'ncbi_pdid', 'link_action_name')\n",
    "\n",
    "# here we loop through each visits, test if any of the 4 conditions are met, and assign the label as success or failuer\n",
    "labels = {}\n",
    "# save each first anchor page to consolidate core data for model\n",
    "indeces = []\n",
    "visits = list(set(data['vid']))\n",
    "data.sort_values(['vid', 'datetime'], inplace=True)\n",
    "ct = 0\n",
    "successes = [0, 0, 0, 0]\n",
    "for visit in visits:\n",
    "    temp = data[data['vid'] == visit]\n",
    "    # this try statement catches mal-indexed visits that don't have anchor page even though they all should\n",
    "    # this is from background errors we sometimes receive from the AppLog client\n",
    "    try:\n",
    "        idx = temp[temp['anchor_page'] == 'yes'].index.values[0]\n",
    "        ct += 1\n",
    "    except:\n",
    "        # if no anchor page, skip this visit\n",
    "        continue\n",
    "    indeces.append(idx)\n",
    "    df = temp.loc[idx::]\n",
    "    cond1 = df.apply(docsum_fasta, args=(col_names_docsum_fasta), axis=1)\n",
    "    if cond1.sum() > 0:\n",
    "        labels[visit] = 'success'\n",
    "        successes[0] += 1\n",
    "        continue\n",
    "    cond2 = df.apply(genbank_to_fasta, args=(col_names_genbank_fasta), axis=1)\n",
    "    if cond2.sum() > 0:\n",
    "        labels[visit] = 'success'\n",
    "        successes[1] += 1\n",
    "        continue\n",
    "    cond3 = df.apply(download_docsum, args=(col_names_download_docsum), axis=1)\n",
    "    if cond3.sum():\n",
    "        labels[visit] = 'success'\n",
    "        successes[2] += 1\n",
    "        continue\n",
    "    cond4 = df.apply(download_record, args=(col_names_download_record), axis=1)\n",
    "    if cond4.sum():\n",
    "        labels[visit] = 'success'\n",
    "        successes[3] += 1\n",
    "    else:\n",
    "        labels[visit] = 'failure'\n",
    "succ_tot = 0\n",
    "for i in successes:\n",
    "    succ_tot += i\n",
    "summary_print = 'There are ' + str(len(labels) - succ_tot) + ' unsuccessful visits and '\n",
    "summary_print += str(succ_tot) + ' successful visits'\n",
    "print(summary_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Final Data Cleanup and Simplification, Quantify Numeric Vectors, Generate [X, y] and Perform Train-Validate-Test Split\n",
    "We do some final data cleanup to remove non-labeled visits, only use one row per visit, which comes for the initial search in the visit.\n",
    "\n",
    "**A note on categorical variables**: While sci-kit learn requires numeric data and normally one would one-hot encode, in our case the categorical variables *do* have ordinal relationships, so rather than creating binary vectors for each category level, we use numerical relationships.  So we use [1,2,3] in lieu of ['low', 'med', 'high'].  It is trivial enough to use one-hot encoding for future analyses if necessary.\n",
    "\n",
    "Finally, only our created feature columns are needed for our feature matrix X; we then split our data randomly into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# only include the first \"anchor\" search in each visit to generate the model\n",
    "# we stored the indeces in a list of the same name from the previous step\n",
    "final_df = data[data.index.isin(indeces)]\n",
    "final_df['label'] = final_df['vid'].map(labels).fillna('NoLabel')\n",
    "\n",
    "# ensure no unlabeled data in matrix (this happens from errors in the initial dataset)\n",
    "final_df = final_df[final_df['label'] != 'NoLabel']\n",
    "\n",
    "# balance the number of successful vs failed visits \n",
    "corr = final_df[final_df['label'] == 'success']\n",
    "wrong = final_df[final_df['label'] == 'failure']\n",
    "wrong = wrong.sample(corr.shape[0])\n",
    "frames = [corr, wrong]\n",
    "final_df = pd.concat(frames)\n",
    "\n",
    "# extract labels and convert to binary numeric\n",
    "y = final_df['label']\n",
    "y_labels = y\n",
    "y = [1 if i == 'success' else 0 for i in y]\n",
    "\n",
    "# remove label and make the visit ID the index value since it is not used in the model\n",
    "final_df.index = final_df['vid']\n",
    "final_df = final_df[['org', 'gene', 'bool', 'bio_term', 'term_count', 'length', 'result_count', 'has_acc', 'has_id', 'has_seq', 'has_tax']]\n",
    "\n",
    "# convert categorical, ordinal features to numeric - note this is NOT one-hot encoding! (see above)\n",
    "for col in ['term_count', 'length', 'result_count']:\n",
    "    final_df[col] = final_df[col].map({'low': 1, 'med': 2, 'high': 3})\n",
    "\n",
    "# convert boolean vectors to binary numeric\n",
    "for col in final_df:\n",
    "    if col not in ['term_count', 'length', 'result_count']:\n",
    "        final_df[col] = final_df[col].map({True: 1, False: 0})\n",
    "\n",
    "# split training and test with 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df, y, test_size=0.3, stratify=y)\n",
    "print('The final model size for our training set is ' + str(X_train.shape[0]) + ' by ' + str(X_train.shape[1]))\n",
    "print(str(sum(y_train)) + ' of these ' + str(X_train.shape[0]) + ' training samples are successful')\n",
    "\n",
    "# convert numeric labels back to strings for plots later\n",
    "y_labels = ['success' if i == 1 else 'failure' for i in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Build Decision Tree Model\n",
    "We want to classify a successful visit by features we find as a result of the user's first search in the visit.  This is a supervised classification problem that does not involve regression.  For interpretaibility, we will apply a decision tree, which will allow us to see what the most important features are.  They are also fairly speedy.  We will see if the tree depth has any effect on the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model using decision tree class\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "for depth in [3, 5, 7]:\n",
    "    dtc = DecisionTreeClassifier(random_state=21, max_depth=depth)\n",
    "    dtc.fit(X_train, y_train)\n",
    "\n",
    "    # store predicted values and calculate accuracy\n",
    "    y_pred = dtc.predict(X_test)\n",
    "    print('classification score with a depth of ' + str(depth) + ' is ' + str(dtc.score(X=X_test, y=y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Evaluate Model Metrics\n",
    "We can use standard metrics libraries from sci-kit learn to assess model peformance.  Most commonly, accuracy and precision and recall values from the classification report are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate sci-kit learn metrics for decision tree classification matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy and performance of Decision Tree Classifier with ' + str(depth) + ' layers:')\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Visualize Model with Graphviz\n",
    "The plot type allows us to examine the decision nodes from the tree model easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use graphviz to visualize trees\n",
    "import graphviz\n",
    "from sklearn import tree\n",
    "# convert training labels back to original categories\n",
    "dot_data = tree.export_graphviz(dtc, out_file=None, feature_names=X_train.columns,\n",
    "                      class_names=y_labels, filled=True, rounded=True, special_characters=True)  \n",
    "graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Results\n",
    "From an analysis of 50,000 visits, there are 29173 unsuccessful visits and 11737 successful visits\n",
    "\n",
    "The final model size for our training set contained16431 by 11\n",
    "8215 of these 16431 training samples are successful\n",
    "Accuracy and performance of Decision Tree Classifier with 3 layers:\n",
    "\n",
    "Confusion Matrix:\n",
    "[[1809 1712]\n",
    " [1697 1825]]\n",
    "\n",
    "\n",
    "Classification report:\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.52      0.51      0.51      3521\n",
    "          1       0.52      0.52      0.52      3522\n",
    "\n",
    "avg / total       0.52      0.52      0.52      7043\n",
    "\n",
    "Accuracy: 0.5159733068294761\n",
    "\n",
    "\n",
    "The model was used with 5 and 7 layers as well, but they did not affect the performance.  It is clear from these scores that the features examined here are not capable of predicting visit behavior, but it doesn't rule out the possibility of it being used when survey results are returned to use the real \"ground truth\" labels for when a user has a successful visit.  We also have additional, more complicated features to test out as well.\n",
    "\n",
    "A random forest model, which could have better peformance because it is less susceptible to sample biasing, gave similar results.  This suggests that the model features are the real issue, rather than model selection.  Those results are provided below, using 20 estimators.  We also looked at estimators of 5 and 10, and neither affected the outcome (results not shown here).\n",
    "\n",
    "Accuracy and performance of Random Forests on non-normalized data with 20 estimators:\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "[[2059 1462]\n",
    " [1865 1657]]\n",
    " \n",
    "Classification report:\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.52      0.58      0.55      3521\n",
    "          1       0.53      0.47      0.50      3522\n",
    "\n",
    "avg / total       0.53      0.53      0.53      7043\n",
    "\n",
    "Accuracy: 0.5276160726962942"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplemental: Training a Random Forest Model\n",
    "Decision trees are great for interpretation, are fast, and can handle numerical and categorical data.  But these advanatages come with caveats; they are prone to biases and overfitting.  Random Forests use sampling to reduce bias and overfitting.  To use Random Forests, we would need to perform one-hot encoding, and we would also want to see the effect of scaling\n",
    "\n",
    "We will perform these steps below to generare a Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import class for random forest calssifcation and apply on train data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=20, random_state=21)  \n",
    "rfc.fit(X_train, y_train)  \n",
    "y_pred = rfc.predict(X_test)\n",
    "print(rfc.score(X_test, y_test))\n",
    "\n",
    "# repeat but apply scaling to X_train and X_test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  \n",
    "X_train_sc = scaler.fit_transform(X_train)  \n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=20, random_state=21)  \n",
    "rfc.fit(X_train_sc, y_train)  \n",
    "y_pred_sc = rfc.predict(X_test_sc)\n",
    "rfc.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling seems to have no effect on the random forest model, perhaps due to all of the features being cetgorical, and thus not having variance differences like continuous features would"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model using precision-recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again we leverage the metrics from sci-kit learn\n",
    "print('Accuracy and performance of Random Forests on non-normalized data with 20 estimators:')\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_test, y_pred)\n",
    "%matplotlib inline\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve on Random Forest Classifier Using Normalized Data AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
